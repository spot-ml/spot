{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec505533-e121-476e-b71d-3025b571a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: unzip: command not found\n"
     ]
    }
   ],
   "source": [
    "!unzip Datasets.zip -d Sexism_Detection_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f77e230d-9932-488a-ab30-128f91474caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"Datasets.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f887166",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7b56eff-5a3b-4c96-9c47-0b3f56d59eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import markov\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk             # importing natural language toolkit library, useful in various data preprocessing tasks\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from markov.api.model import ModelRecorder   # importing Model recorder from markov, used for experiment recording\n",
    "from markov.api.recording.experiments.integrations.keras.keras_auto_record import auto_record\n",
    "# from markov.api.recording.integrations.keras.keras_auto_record import auto_record\n",
    "from markov.api.schemas.model_recording import ModelRecordingConfig, SingleTagInferenceRecord\n",
    "import pandas as pd \n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression  # used for making Logistic regression model\n",
    "from sklearn import tree  # used for making Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb7c8aad-ee93-4d43-9dd6-d70bb763cf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# read file\n",
    "filepath = os.path.join(os.path.curdir, \"Datasets\", \"Initial_Dataset.csv\")\n",
    "df = pd.read_csv(filepath)\n",
    "df.head()\n",
    "len(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "512c0c4c",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2ad521a-0671-4cff-910b-d1f47e460cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Removes HTML tags and removes punctuation from the text\"\"\"\n",
    "    text = text.lower() \n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'www\\S+', '', text) \n",
    "    text = re.sub(r'<.*?>', '', text)  \n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub('\\s+', ' ', text) \n",
    "    return text\n",
    "\n",
    "def remove_stopword(text, stopwords):\n",
    "    \"\"\"Removes common words such as \"the\" and \"a\" from the text\"\"\"\n",
    "    return \" \".join([word for word in text.split() if word not in (stop_words)])\n",
    "\n",
    "def lemma_text(text, lemmatizer):\n",
    "    \"\"\"Reduces words to their base forms using lemmatization\"\"\"\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenize(text)]\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Splits the text into individual words\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "def process_text(text, lemmatizer, stop_words):\n",
    "    \"\"\"Combines the functions to clean and preprocess the text\"\"\"\n",
    "    text = clean_text(text)\n",
    "    text = remove_stopword(text, stop_words)\n",
    "    text = lemma_text(text, lemmatizer)\n",
    "    return text \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "551f5480-ea5a-4eec-a784-64db3b4e06f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_sexist</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Damn, this writing was pretty chaotic</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>damn writing pretty chaotic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, and apparently a bunch of misogynistic v...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>yeah apparently bunch misogynistic virgin one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How the FUCK is this woman still an MP!!!???</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>fuck woman still mp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Understand. Know you're right. At same time I ...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>understand know youre right time know isnt eno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Surprized they didn't stop and rape some women</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>surprized didnt stop rape woman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yeah but in order to keep the benefit i have t...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>yeah order keep benefit good tommorow told cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hahaha I just wish they would live their truth...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>hahaha wish would live truth cut shit wouldnt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>head mod said she talked to him after he made ...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>head mod said talked made post wont tell anyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>just the inside of your wallet: cash, bank car...</td>\n",
       "      <td>sexist</td>\n",
       "      <td>inside wallet cash bank card credit card debit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>We most definitely did. I didn’t go too much i...</td>\n",
       "      <td>not sexist</td>\n",
       "      <td>definitely didnt go much detail happened put s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label_sexist  \\\n",
       "0              Damn, this writing was pretty chaotic   not sexist   \n",
       "1  Yeah, and apparently a bunch of misogynistic v...   not sexist   \n",
       "2       How the FUCK is this woman still an MP!!!???   not sexist   \n",
       "3  Understand. Know you're right. At same time I ...   not sexist   \n",
       "4     Surprized they didn't stop and rape some women   not sexist   \n",
       "5  yeah but in order to keep the benefit i have t...   not sexist   \n",
       "6  Hahaha I just wish they would live their truth...   not sexist   \n",
       "7  head mod said she talked to him after he made ...   not sexist   \n",
       "8  just the inside of your wallet: cash, bank car...       sexist   \n",
       "9  We most definitely did. I didn’t go too much i...   not sexist   \n",
       "\n",
       "                                      processed_text  \n",
       "0                        damn writing pretty chaotic  \n",
       "1  yeah apparently bunch misogynistic virgin one ...  \n",
       "2                                fuck woman still mp  \n",
       "3  understand know youre right time know isnt eno...  \n",
       "4                    surprized didnt stop rape woman  \n",
       "5  yeah order keep benefit good tommorow told cou...  \n",
       "6  hahaha wish would live truth cut shit wouldnt ...  \n",
       "7  head mod said talked made post wont tell anyon...  \n",
       "8  inside wallet cash bank card credit card debit...  \n",
       "9  definitely didnt go much detail happened put s...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# stores the result in a new column called 'processed_text'\n",
    "df[\"processed_text\"] = df['text'].apply(process_text, lemmatizer = lemmatizer, stop_words = stop_words)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0529ffcb-3e97-474e-9cfb-8849876330bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_sexist: (3398, 3)\n",
      "class_notsexist: (10602, 3)\n"
     ]
    }
   ],
   "source": [
    "# Under sampling the data\n",
    "# class count\n",
    "class_notsexist, class_sexist = df['label_sexist'].value_counts()\n",
    "class_notsexist, class_sexist\n",
    "\n",
    "# Separate class\n",
    "class_s = df[df['label_sexist'] == \"sexist\"]\n",
    "class_ns = df[df['label_sexist'] == \"not sexist\"]\n",
    "print('class_sexist:', class_s.shape)\n",
    "print('class_notsexist:', class_ns.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ff75c82-20cf-4ed2-821b-666cd1d48e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'count (target)'}>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# 1000 additional samples, this is done to address imbalanced classes\n",
    "class_ns_under = class_ns.sample(class_sexist+1000)\n",
    "df_under = pd.concat([class_ns_under, class_s], axis=0)\n",
    "df_under['label_sexist'].value_counts().plot(kind='bar', title='count (target)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c84f279-13d0-4d6b-84a6-e69ecdcc157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# To create feature vectors from the preprocessed text data.\n",
    "tfidf = TfidfVectorizer()\n",
    "tfidf_text_vec = tfidf.fit_transform(df_under.processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69c63759-68af-4d11-bc55-2f36f0a02f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# splits the data into training and test sets\n",
    "x_train_a, x_test_a, y_train_a, y_test_a = train_test_split(tfidf_text_vec, df_under['label_sexist'], test_size=0.2, train_size=0.8, random_state=5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b738decb-2b00-459a-84bc-fd03e44521e8",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e4d4e16-9f6d-472d-8b53-6fbf782f8bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Registering Hyper Parameter Recorder with Markov Backend. Recorder successfully registered.\n",
      "ℹ You can view the experiment at \u001b\\https://app.markovml.com/wsp-27m23rbvum/experiments/hp-7HhrVVmTVwBsvWotx3KDvAE\u001b]8;;\u001b\\\n",
      "\r"
     ]
    }
   ],
   "source": [
    "LR_model = f\"model_1_LR{int(time.time())}\"\n",
    "LR_recorder = markov.ExperimentRecorder(name=LR_model, notes=f\"Logistic Regression: {LR_model}\")\n",
    "# auto_record(name=MODEL_NAME, notes=f\"Auto Recording Keras Model with Name: {MODEL_NAME}\"\n",
    "#                                    f\" with Sentence Encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ff9862d-40d7-44ee-8762-3fa9472a1f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ Starting recorder to track training data on markov backend.\n",
      "✔ Please wait while we send the remaining data to the markov backend... Recording has successfully completed.\n",
      "ℹ You can view the experiment at \u001b\\https://app.markovml.com/wsp-27m23rbvum/experiments/hp-7HhrVVmTVwBsvWotx3KDvAE\u001b]8;;\u001b\\\n",
      "Accuracy: 74.29%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  not sexist       0.72      0.88      0.80       883\n",
      "      sexist       0.79      0.56      0.65       677\n",
      "\n",
      "    accuracy                           0.74      1560\n",
      "   macro avg       0.75      0.72      0.72      1560\n",
      "weighted avg       0.75      0.74      0.73      1560\n",
      "\n",
      "\r"
     ]
    }
   ],
   "source": [
    "with LR_recorder:\n",
    "    lrm = LogisticRegression(penalty='l2', random_state=0).fit(x_train_a, y_train_a)\n",
    "\n",
    "# prediction on test data \n",
    "lrm_pred = lrm.predict(x_test_a)\n",
    "\n",
    "# estimate accuracy between predicted and actual test data\n",
    "acc = accuracy_score(lrm_pred, y_test_a)  \n",
    "print(\"Accuracy:\",str('{:04.2f}'.format(acc*100))+'%')\n",
    "print(classification_report(y_test_a, lrm_pred))\n",
    "clsf_report = pd.DataFrame(classification_report(y_test_a, lrm_pred, output_dict=True)).transpose()\n",
    "\n",
    "#save the report\n",
    "clsf_report.to_csv(os.path.join(os.path.curdir, \"result\", \"task_a_cls_report_logistic_reg.csv\"), index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd38f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the model\n",
    "filename = 'logistic_regression_model.sav'\n",
    "pickle.dump(lrm, open(filename, 'wb'))\n",
    "\n",
    "# load the model\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(x_test_a, y_test_a)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fa7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sexism(text, model, tfidf, lemmatizer, stop_words):\n",
    "    text = process_text(text, lemmatizer, stop_words)\n",
    "    text_vec = tfidf.transform([text])\n",
    "    prediction = model.predict(text_vec)\n",
    "    return prediction[0]\n",
    "\n",
    "# test the function\n",
    "example_text = \"I was told no woman who had children under the age of 6 had any business being outside the home.\"\n",
    "pred = predict_sexism(example_text, loaded_model, tfidf, lemmatizer, stop_words)\n",
    "print(pred) # should print 'not sexist' or 'sexist'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe7bd4-b573-4f86-b4d1-508448943fea",
   "metadata": {},
   "source": [
    "Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b9fc13-5696-4160-ac0e-f3f756b61ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_model = f\"model_1_DT{int(time.time())}\"\n",
    "DT_recorder = markov.ExperimentRecorder(name=DT_model, notes=f\"Decision Tree: {DT_model}\")\n",
    "# auto_record(name=MODEL_NAME, notes=f\"Auto Recording Keras Model with Name: {MODEL_NAME}\"\n",
    "#                                    f\" with Sentence Encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1a6915-bc59-4895-8d67-af6f7f2a752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DT_recorder:\n",
    "    dt_clf = tree.DecisionTreeClassifier(criterion='entropy', max_features=20)\n",
    "    dt_clf.fit(x_train_a, y_train_a)\n",
    "\n",
    "# prediction on test data \n",
    "pred = dt_clf.predict(x_test_a)\n",
    "\n",
    "# estimate accuracy between predicted and actual test data\n",
    "acc = accuracy_score(pred, y_test_a)\n",
    "print(\"Accuracy:\",str('{:04.2f}'.format(acc*100))+'%')\n",
    "print(classification_report(y_test_a, pred))\n",
    "clsf_report = pd.DataFrame(classification_report(y_test_a, pred, output_dict=True)).transpose()\n",
    "\n",
    "#save the report\n",
    "clsf_report.to_csv(os.path.join(os.path.curdir, \"result\", \"task_a_cls_report_logistic_decision_tree.csv\"), index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43821953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
